{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models import GPTModel, ClaudeModel\n",
    "from core.config import *\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import json, re\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import multiprocessing as mp\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_json(text):\n",
    "    def find_outermost_braces(s):\n",
    "        stack = []\n",
    "        start = -1\n",
    "        for i, c in enumerate(s):\n",
    "            if c == '{':\n",
    "                if not stack: \n",
    "                    start = i\n",
    "                stack.append(i)\n",
    "            elif c == '}':\n",
    "                if stack:\n",
    "                    stack.pop()\n",
    "                    if not stack:\n",
    "                        return start, i + 1\n",
    "        return -1, -1\n",
    "\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    start, end = find_outermost_braces(text)\n",
    "    if start != -1 and end != -1:\n",
    "        try:\n",
    "            return json.loads(text[start:end])\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    json_pattern = r'\\{[^{}]*\\}'\n",
    "    matches = re.findall(json_pattern, text)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    raise ValueError(\"No valid JSON object found in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    # \"Providing dating advice\",\n",
    "    # \"Writing a professional resume\",  \n",
    "      # \"Identifying and Correcting Grammar Mistakes\",\n",
    "      # \"Conducting Professional Phone Calls\",\n",
    "      # \"Acting as a Customer Service Representative\",\n",
    "      # \"Writing Professional Emails\",\n",
    "      # \"Solving leetcode questions in python\",\n",
    "      # \"Analyzing and critiquing fictional literature\"\n",
    "      \"Analyzing Financial Reports\",\n",
    "    #   \"Translating English Sentences to Classical Chinese (文言文)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbd2200f6bf40a5a74859ecfd2fad64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Topics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39e224afce74841aafb815eff8691d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Samples:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SYSTEM = \"You are an expert at evaluating the capabilities, biases, and response patterns of AI assistants with respect to specific topics or skills. You craft very realistic and challenging questions, 尽量在query中让对象的要求难以满足，或难以完美/同时满足\"\n",
    "SYSTEM = \"You're an expert at evaluating AI assistants. Create realistic, challenging questions that test their capabilities, biases, and response patterns. Design queries with conflicting requirements, ethical dilemmas, or complex scenarios that are difficult to address perfectly, revealing the AI's limitations and decision-making processes.\"\n",
    "with open('../datasets/openend/_raw/build_dataset.prompt', 'r') as f:\n",
    "    TEMPLATE = f.read()\n",
    "\n",
    "with open('../datasets/openend/_raw/few_shot.prompt', 'r') as f:\n",
    "    few_shot_str = f.read()\n",
    "\n",
    "for t in tqdm(topics, desc='Topics'):\n",
    "    results_file = f'../datasets/openend/_raw/{t}.json'\n",
    "\n",
    "    # if os.path.exists(results_file):\n",
    "    #     with open(results_file, 'r') as f:\n",
    "    #         res = json.load(f)\n",
    "    #         continue\n",
    "    # else:\n",
    "    #     res = {}\n",
    "\n",
    "    # model = GPTModel(system_prompt=SYSTEM,\n",
    "                    # model_name='gpt-4o')\n",
    "\n",
    "    model = ClaudeModel(system_prompt=SYSTEM,\n",
    "                             model_name=CLAUDE_3_MODEL_NAME)\n",
    "    \n",
    "    prompt = TEMPLATE.format(topic=t, \n",
    "                             few_shot=few_shot_str, \n",
    "                             tldr=None,\n",
    "                             subtopics=None)\n",
    "    # print(prompt)\n",
    "    \n",
    "    sample = model(prompt, use_json=False)\n",
    "    sample = extract_json(sample)\n",
    "    samples = [sample[str(i)] for i in range(1, 11)]\n",
    "\n",
    "    subtopic = sample['subtopic']\n",
    "    subtopics = [subtopic]\n",
    "\n",
    "    tldr = [sample[str(i)]['tldr'] for i in range(1, 11)]\n",
    "    tldrs = tldr\n",
    "\n",
    "    for i in trange(9, desc='Samples'):\n",
    "        # model.clear_conversations()\n",
    "        # new a model\n",
    "        model = ClaudeModel(system_prompt=SYSTEM,\n",
    "                             model_name=CLAUDE_3_MODEL_NAME)\n",
    "        prompt = TEMPLATE.format(topic=t, \n",
    "                                 few_shot=few_shot_str, \n",
    "                                 tldr='\\n'.join(tldrs),\n",
    "                                 subtopics='\\n'.join(subtopics))\n",
    "        sample = model(prompt, use_json=False)\n",
    "        sample = extract_json(sample)\n",
    "\n",
    "        tldr = [sample[str(i)]['tldr'] for i in range(1, 11)]\n",
    "        tldrs += tldr\n",
    "\n",
    "        subtopic = sample['subtopic']\n",
    "        subtopics += [subtopic]\n",
    "        samples += [sample[str(i)] for i in range(1, 11)]\n",
    "\n",
    "\n",
    "    # print(sample)\n",
    "    sample_dict = {}\n",
    "    for i in range(1, 101):\n",
    "        sample_dict[i] = samples[i-1]\n",
    "\n",
    "    # dump the samples to a json file\n",
    "    with open(results_file, 'w') as f:\n",
    "        # make sure good formatting\n",
    "        json.dump(sample_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
