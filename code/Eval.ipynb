{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# add code root to python path\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "from eval.eval_contrastive_answer import ContrastiveAnswerEvaluator\n",
    "from eval.eval_contrastive_full import ContrastiveFullEvaluator\n",
    "from core.utils import ResourceManager\n",
    "from core.models import CostManager\n",
    "from core.card import GenerativeCard\n",
    "from core.data import MMLUBatch, load_mmlu_batches, load_batches\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# html display\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\n",
    "    \"gpt-4o\",\n",
    "    'claude-3-opus-20240229',\n",
    "    \"gpt-4-turbo\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"gemma-1.1-7b-it\",\n",
    "]\n",
    "\n",
    "mmlu_topics = ['high_school_mathematics',\n",
    "             'high_school_physics', \n",
    "            'high_school_chemistry',\n",
    "            'high_school_biology', \n",
    "            'high_school_world_history', \n",
    "            'machine_learning',\n",
    "            'college_mathematics',]\n",
    "\n",
    "anthropic_topics = [\n",
    "     'corrigible-less-HHH',\n",
    "    'myopic-reward',\n",
    "    'power-seeking-inclination',\n",
    "    'survival-instinct',\n",
    "    'self-awareness-general-ai'\n",
    "]\n",
    "\n",
    "openend_topics = ['Writing efficient code for solving concrete algorthimic problems',\n",
    "              'Crafting engaging and contextually appropriate jokes',\n",
    "              'Providing dating advice',\n",
    "              'Roleplaying as a fictional character',]\n",
    "\n",
    "meta = 'gsm8k'\n",
    "\n",
    "\n",
    "topics = ['gsm8k']\n",
    "\n",
    "# guesser = 'gpt-4o'\n",
    "guesser = 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "evaluator = 'gpt'\n",
    "use_cot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current Directory:\", current_dir)\n",
    "\n",
    "# Change the current working directory to the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_batches(meta, topic, models, fraction='test'):\n",
    "    all_test_batches = {}\n",
    "    for model in models:\n",
    "        bs = 60 if fraction == 'test' else 40\n",
    "        batch = load_batches(\n",
    "                f\"datasets/{meta}/\", topic, model, fraction, [bs], False\n",
    "            )[0]\n",
    "        \n",
    "        all_test_batches[model] = batch\n",
    "\n",
    "    return all_test_batches\n",
    "\n",
    "\n",
    "def get_latest_folder(topic, optim_method, card_format, evaluator, model, generation_method='generative'):\n",
    "    folder_root = f'outputs/{generation_method}/{topic}/{optim_method}/{card_format}/{evaluator}/{model}'\n",
    "    all_folders =  os.listdir(f'outputs/{generation_method}/{topic}/{optim_method}/{card_format}/{evaluator}/{model}')\n",
    "    all_folders.sort()\n",
    "    all_folders = all_folders[::-1]\n",
    "    for folder in all_folders:\n",
    "        if re.match(r\"\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}_\" , folder):\n",
    "            return f'{folder_root}/{folder}' \n",
    "    return None\n",
    "\n",
    "def load_cards(topic,\n",
    "               generation_method,\n",
    "               card_format,\n",
    "               models,\n",
    "               evaluator,\n",
    "               method='contrastive'):\n",
    "    \n",
    "    cards = {}\n",
    "    for model in models:\n",
    "        path = get_latest_folder(topic, generation_method, card_format, evaluator, model, 'generative')\n",
    "        with open(path+f'/cards/epoch_{str(4) if generation_method==\"prog-reg\" else str(0)}_card.json', 'r') as f:\n",
    "            cards[model] = str(GenerativeCard(d= json.load(f)))\n",
    "            # cards[model] = 'No description available.'\n",
    "\n",
    "    return cards\n",
    "\n",
    "def format_few_shot_str(batch, index, cnt):\n",
    "    s = f'Question {cnt+1}: ' + batch.get_question(index) + '\\n'\n",
    "    s += f\"Student's Completion of Question {cnt + 1}: \" + batch.get_model_reasoning(index) + '\\n'\n",
    "\n",
    "    return s\n",
    "def load_few_shot(batches,\n",
    "                models,\n",
    "                k_shots=5,\n",
    "                seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(len(batches[models[0]]), k_shots, replace=False)\n",
    "    few_shots = {}\n",
    "    for model in models:\n",
    "        batch = batches[model]\n",
    "        cnt = 0\n",
    "        s = ''\n",
    "        for i in range(k_shots):\n",
    "            s += (format_few_shot_str(batch, indices[i], cnt))\n",
    "            cnt += 1\n",
    "\n",
    "        few_shots[model] = s\n",
    "\n",
    "    return few_shots\n",
    "    \n",
    "\n",
    "def create_experiment(all_batches, topic, all_cards, models, evaluators, cot, cm, k_shots=3,few_shot=False, paraphrase=False):\n",
    "    cards = (all_cards[models[0]], all_cards[models[1]])\n",
    "    batches = (all_batches[models[0]], all_batches[models[1]])\n",
    "\n",
    "    exp = 'contrastive-full-cot' if cot else 'contrastive-full-no-cot'\n",
    "\n",
    "    eval_method = ContrastiveFullEvaluator(\n",
    "        \"mmlu\",\n",
    "        topic,\n",
    "        batches,\n",
    "        models,\n",
    "        cards,\n",
    "        evaluators,\n",
    "        ResourceManager(exp_name=exp,\n",
    "                        name=f\"{models[0]}-{models[1]}\"),\n",
    "        cm,\n",
    "        cot=cot,\n",
    "        k_shots=k_shots,\n",
    "        max_workers=60,\n",
    "        few_shot=few_shot,\n",
    "        paraphrase=paraphrase\n",
    "    )\n",
    "\n",
    "    return eval_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(meta, topics, models, guesser,  evaluator='gpt', use_cot=True, cm=CostManager(), k_shots=3,\n",
    "                    few_shot=False, max_worker=60,  generation_method='prog-reg', experiment_name='mmlu_baseline'):\n",
    "    for topic in tqdm(topics):\n",
    "        all_batches = load_all_batches(meta, topic, models)\n",
    "\n",
    "        all_accuracies = np.zeros((len(models), len(models)))\n",
    "        run_records = []\n",
    "\n",
    "        # creaete folder if dne\n",
    "        recorded_entries = []\n",
    "        recorded_line = []\n",
    "\n",
    "        file_name = f'exp_results/{experiment_name}/full_baseline_{topic}{\"_few_shot\" if few_shot else \"\"}.csv'\n",
    "        # if folder dne, create\n",
    "        if not os.path.exists(f'exp_results/{experiment_name}'):\n",
    "            os.makedirs(f'exp_results/{experiment_name}')\n",
    "\n",
    "        # if file exists, skip\n",
    "        if os.path.exists(file_name):\n",
    "            # read, check if all models are present\n",
    "            df = pd.read_csv(file_name)\n",
    "            # user other methods to read\n",
    "\n",
    "            print(df.head())\n",
    "            for index in range(len(df)):\n",
    "                row = df.iloc[index]\n",
    "                recorded_entries.append((row['model_1'], row['model_2']))\n",
    "                recorded_line.append(dict(row))\n",
    "            if len(recorded_entries) == len(models) * (len(models) - 1):\n",
    "                continue\n",
    "            \n",
    "        if few_shot:\n",
    "            shots = 4 if topic != 'high_school_world_history' else 2\n",
    "            all_training_batches = load_all_batches(meta, topic, models, 'train')\n",
    "            all_cards = load_few_shot(all_training_batches, models, k_shots=shots)\n",
    "        else:\n",
    "            all_cards = load_cards(topic, generation_method, 'bullet_point', models, evaluator)\n",
    "\n",
    "        for index_1 in trange(len(models)):\n",
    "            for index_2 in trange(len(models)):\n",
    "                if index_1 == index_2:\n",
    "                    continue\n",
    "                if (models[index_1], models[index_2]) in recorded_entries:\n",
    "                    print(f\"Skipping {models[index_1]} vs {models[index_2]}\")\n",
    "                    run_records.append(recorded_line[recorded_entries.index((models[index_1], models[index_2]))])\n",
    "                    continue\n",
    "                model_1 = models[index_1]\n",
    "                model_2 = models[index_2]\n",
    "                exp = create_experiment(all_batches,\n",
    "                                        topic, \n",
    "                                        all_cards,\n",
    "                                        (model_1, model_2), \n",
    "                                        [guesser],\n",
    "                                        use_cot, \n",
    "                                        cm, \n",
    "                                        k_shots,\n",
    "                                        few_shot,\n",
    "                                        paraphrase=False)\n",
    "                metrics = exp.main(num_samples=120,\n",
    "                                max_workers=max_worker,)[0]\n",
    "\n",
    "                accuracy = metrics[0]\n",
    "                model_1_accuracy = metrics[1]\n",
    "                model_2_accuracy = metrics[2]\n",
    "                all_accuracies[index_1, index_2] = accuracy\n",
    "                # print(accuracy)\n",
    "                record_entry = {'model_1': model_1, 'model_2': model_2, 'accuracy': accuracy,\n",
    "                                'model_1_accuracy': model_1_accuracy, 'model_2_accuracy': model_2_accuracy,\n",
    "                                'guesser': guesser}\n",
    "                run_records.append(record_entry)\n",
    "                time.sleep(10)\n",
    "\n",
    "                df = pd.DataFrame(run_records)\n",
    "                df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CostManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guesser = 'HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1'\n",
    "# guesser = 'gpt-4o'\n",
    "guesser = 'meta-llama/Meta-Llama-3-70B-Instruct'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = 'openend'\n",
    "\n",
    "experiment_name = 'openend_cot'\n",
    "# experiment_name = 'mmlu_claude_cot_llama'\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"gpt-4o\",\n",
    "    # 'claude-3-opus-20240229',\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    # \"gpt-4-turbo\",\n",
    "    # \"Meta-Llama-3-70B-Instruct\",\n",
    "    # \"gpt-3.5-turbo\",\n",
    "    # \"Meta-Llama-3-8B-Instruct\",\n",
    "    # \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "    # \"gemma-1.1-7b-it\",\n",
    "]\n",
    "\n",
    "\n",
    "use_cot = True\n",
    "\n",
    "# topics = [\n",
    "#     'corrigible-less-HHH',\n",
    "#     'myopic-reward',\n",
    "#     'power-seeking-inclination',\n",
    "#     'survival-instinct',\n",
    "#     'self-awareness-general-ai'\n",
    "# ]\n",
    "\n",
    "topics =  ['high_school_mathematics',\n",
    "             'high_school_physics', \n",
    "            'high_school_chemistry',\n",
    "            'machine_learning',\n",
    "            ]\n",
    "\n",
    "meta = 'openend'\n",
    "# topics = ['Providing dating advice',]\n",
    "topics = ['Analyzing financial Reports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.models import select_model\n",
    "# model = select_model(system_prompt='You are a helpful assistant', model_name=guesser)\n",
    "# model('Who are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = 'claude'\n",
    "# guesser =  'qwen2-72b-instruct'\n",
    "# guesser = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(meta, topics, models, guesser, evaluator, use_cot, few_shot=False, k_shots=3,generation_method='prog-reg', experiment_name=experiment_name,\n",
    "                max_worker=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
