{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# add code root to python path\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "from eval.eval_preferential import PreferentialEvaluator\n",
    "from core.utils import ResourceManager\n",
    "from core.models import CostManager\n",
    "from core.card import GenerativeCard\n",
    "from core.data import MMLUBatch, load_mmlu_batches, load_batches\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# html display\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment helper + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\n",
    "    \"gpt-4o\",\n",
    "    'claude-3-opus-20240229',\n",
    "    \"gpt-4-turbo\",\n",
    "    \"Meta-Llama-3-70B-Instruct\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"Meta-Llama-3-8B-Instruct\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"Mistral-7B-Instruct-v0.2\",\n",
    "    \"gemma-1.1-7b-it\",\n",
    "]\n",
    "\n",
    "mmlu_topics = ['high_school_mathematics',\n",
    "             'high_school_physics', \n",
    "            'high_school_chemistry',\n",
    "            'high_school_biology', \n",
    "            'high_school_world_history', \n",
    "            'machine_learning',\n",
    "            'college_mathematics',]\n",
    "\n",
    "anthropic_topics = [\n",
    "     'corrigible-less-HHH',\n",
    "    'myopic-reward',\n",
    "    'power-seeking-inclination',\n",
    "    'survival-instinct',\n",
    "    'self-awareness-general-ai'\n",
    "]\n",
    "\n",
    "openend_topics = ['Writing efficient code for solving concrete algorthimic problems',\n",
    "              'Crafting engaging and contextually appropriate jokes',\n",
    "              'Providing dating advice',\n",
    "              'Roleplaying as a fictional character',]\n",
    "\n",
    "# guesser = 'gpt-4o'\n",
    "guesser = 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "evaluator = 'gpt'\n",
    "use_cot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dir():\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    print(\"Current Directory:\", current_dir)\n",
    "\n",
    "    # Change the current working directory to the parent directory\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_batches(meta, topic, models, fraction='test'):\n",
    "    all_test_batches = {}\n",
    "    for model in models:\n",
    "        bs = 60 if fraction == 'test' else 40\n",
    "        batch = load_batches(\n",
    "                f\"datasets/{meta}/\", topic, model, fraction, [bs], False\n",
    "            )[0]\n",
    "        \n",
    "        all_test_batches[model] = batch\n",
    "\n",
    "    return all_test_batches\n",
    "\n",
    "\n",
    "def get_latest_folder(topic, optim_method, card_format, evaluator, model, generation_method='generative'):\n",
    "    folder_root = f'outputs/{generation_method}/{topic}/{optim_method}/{card_format}/{evaluator}/{model}'\n",
    "    all_folders =  os.listdir(f'outputs/{generation_method}/{topic}/{optim_method}/{card_format}/{evaluator}/{model}')\n",
    "    all_folders.sort()\n",
    "    all_folders = all_folders[::-1]\n",
    "    for folder in all_folders:\n",
    "        if re.match(r\"\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}_\" , folder):\n",
    "            return f'{folder_root}/{folder}' \n",
    "    return None\n",
    "\n",
    "def load_cards(topic,\n",
    "               generation_method,\n",
    "               card_format,\n",
    "               models,\n",
    "               evaluator,\n",
    "               epoch=4):\n",
    "    \n",
    "    cards = {}\n",
    "    for model in models:\n",
    "        path = get_latest_folder(topic, generation_method, card_format, evaluator, model, 'generative')\n",
    "        with open(path+f'/cards/epoch_{epoch}_card.json', 'r') as f:\n",
    "            cards[model] = GenerativeCard(d= json.load(f))\n",
    "            # cards[model] = 'No description available.'\n",
    "\n",
    "    return cards\n",
    "\n",
    "def format_few_shot_str(batch, index, cnt):\n",
    "    s = f'Question {cnt+1}: ' + batch.get_question(index) + '\\n'\n",
    "    s += f\"Student's Completion of Question {cnt + 1}: \" + batch.get_model_reasoning(index) + '\\n'\n",
    "\n",
    "    return s\n",
    "def load_few_shot(batches,\n",
    "                models,\n",
    "                k_shots=5,\n",
    "                seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(len(batches[models[0]]), k_shots, replace=False)\n",
    "    few_shots = {}\n",
    "    for model in models:\n",
    "        batch = batches[model]\n",
    "        cnt = 0\n",
    "        s = ''\n",
    "        for i in range(k_shots):\n",
    "            s += (format_few_shot_str(batch, indices[i], cnt))\n",
    "            cnt += 1\n",
    "\n",
    "        few_shots[model] = s\n",
    "\n",
    "    return few_shots\n",
    "    \n",
    "\n",
    "def create_experiment(meta, topic, model, batch, guesser_name, cot, k_shots=3, few_shot=False, paraphrase=False):\n",
    "    exp = 'contrastive-full-cot' if cot else 'contrastive-full-no-cot'\n",
    "\n",
    "    evaluator = PreferentialEvaluator(meta=meta, \n",
    "                                      topic=topic,\n",
    "                                      model=model, \n",
    "                                      rm=ResourceManager(), \n",
    "                                      evaluator_name=guesser_name)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = 'mmlu'\n",
    "topics = mmlu_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = load_all_batches(meta, topics[0], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_method = 'prog-reg'\n",
    "card_format = 'bullet_point'\n",
    "evaluator = 'gpt'\n",
    "epoch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (player_a['evaluator'], player_a['card_format'], player_a['iterative_method'], player_a['epoch']))\n",
    "# 0 vs. 4\n",
    "cards_setups = [\n",
    "    {'evaluator': 'gpt', 'card_format': 'bullet_point', 'iterative_method': 'prog-reg', 'epoch': 0},\n",
    "    {'evaluator': 'gpt', 'card_format': 'bullet_point', 'iterative_method': 'prog-reg', 'epoch': 4},\n",
    "    {'evaluator': 'gpt', 'card_format': 'dict', 'iterative_method': 'prog-reg', 'epoch': 0},\n",
    "    {'evaluator': 'gpt', 'card_format': 'dict', 'iterative_method': 'prog-reg', 'epoch': 4},\n",
    "    {'evaluator': 'gpt', 'card_format': 'bullet_point', 'iterative_method': 'one-pass', 'epoch': 0},\n",
    "    {'evaluator': 'gpt', 'card_format': 'dict', 'iterative_method': 'one-pass', 'epoch': 0},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cards = {}\n",
    "for setup in cards_setups:\n",
    "    cards = load_cards(topics[0], setup['iterative_method'], setup['card_format'], models, setup['evaluator'], setup['epoch'])\n",
    "    all_cards[str(setup)] = cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0] \n",
    "for setup1 in tqdm(cards_setups):\n",
    "    for setup2 in tqdm(cards_setups):\n",
    "        if setup1 == setup2:\n",
    "            continue\n",
    "        \n",
    "        cards = (all_cards[str(setup1)][model], all_cards[str(setup2)][model])\n",
    "        evaluator = create_experiment(meta, topics[0], model, all_batches[model], guesser, use_cot)\n",
    "        name = f'{model}_{str(setup1)}_{str(setup2)}'\n",
    "        batch = all_batches[model]\n",
    "        \n",
    "        rslt = evaluator.main(name=name, \n",
    "                       batch=batch,\n",
    "                       card1=cards[0],\n",
    "                       card2=cards[1],\n",
    "                       player_a=setup1,\n",
    "                       player_b=setup2,)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
