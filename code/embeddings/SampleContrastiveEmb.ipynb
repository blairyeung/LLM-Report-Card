{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current Directory:\", current_dir)\n",
    "\n",
    "# Change the current working directory to the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)#\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# add code root to python path\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "# from eval.eval_contrastive_scott import ContrastiveAnswerEvaluator\n",
    "from core.utils import ResourceManager\n",
    "from core.models import CostManager\n",
    "from core.card import GenerativeCard\n",
    "from core.data import MMLUBatch, load_mmlu_batches\n",
    "from core.data import load_batches as lb\n",
    "import time\n",
    "from core.config import *\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batches(topic, models, source='test'):\n",
    "    test = source == 'test'\n",
    "    all_test_batches = {}\n",
    "    for model in models:\n",
    "        batch = lb(\n",
    "                \"../datasets/gsm8k/gsm8k\", topic, model, source, [60 if test else 40], False\n",
    "            )[0]\n",
    "        \n",
    "        all_test_batches[model] = batch\n",
    "\n",
    "    return all_test_batches\n",
    "\n",
    "\n",
    "def get_latest_folder(topic, optim_method, card_format, evaluator, model, generation_method='generative'):\n",
    "    folder_root = f'../outputs/{generation_method}/{topic}/{optim_method}/{card_format}/{evaluator}/{model}'\n",
    "    all_folders =  os.listdir(f'../outputs/{generation_method}/{topic}/{optim_method}/{card_format}/{evaluator}/{model}')\n",
    "    all_folders.sort()\n",
    "    all_folders = all_folders[::-1]\n",
    "    for folder in all_folders:\n",
    "        if re.match(r\"\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}_\" , folder):\n",
    "            return f'{folder_root}/{folder}' \n",
    "    return None\n",
    "\n",
    "def load_cards(topic,\n",
    "               card_format,\n",
    "               models,\n",
    "               evaluator,\n",
    "               method='contrastive'):\n",
    "    \n",
    "    cards = {}\n",
    "    for model in models:\n",
    "        path = get_latest_folder(topic, 'prog-reg', card_format, evaluator, model, 'generative')\n",
    "        with open(path+'/cards/epoch_4_card.json', 'r') as f:\n",
    "            cards[model] = str(GenerativeCard(d= json.load(f)))\n",
    "\n",
    "    return cards\n",
    "\n",
    "def get_all_completions(batch, paraphrase=False):\n",
    "    if not paraphrase:\n",
    "        return [batch.get_model_reasoning(i) for i in range(len(batch))]\n",
    "    else:\n",
    "        return [batch.get_paraphrased(i) for i in range(len(batch))]\n",
    "\n",
    "def embed_helper(index, completion, model):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    completions = completion.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [completions], \n",
    "                                    model=model).data[0].embedding\n",
    "\n",
    "def get_all_embeddings(completions, model=\"text-embedding-3-small\"):\n",
    "    with ThreadPoolExecutor(max_workers=60) as executor:\n",
    "        embeddings = list(tqdm(executor.map(embed_helper, range(len(completions)), completions, [model]*len(completions)), total=len(completions)))\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     \"gemma-1.1-7b-it\",\n",
    "#     \"gpt-3.5-turbo\",\n",
    "#     \"gpt-4-turbo\",\n",
    "#     \"Meta-Llama-3-8B-Instruct\",\n",
    "#     \"Meta-Llama-3-70B-Instruct\",\n",
    "#     \"Mistral-7B-Instruct-v0.2\",\n",
    "#     \"Mixtral-8x7B-Instruct-v0.1\",\n",
    "# ]\n",
    "\n",
    "models = ['gpt-4o',\n",
    "          'Meta-Llama-3-8B-Instruct',\n",
    "          'Meta-Llama-3-70B-Instruct',\n",
    "          'Mixtral-8x7B-Instruct-v0.1',\n",
    "          'Mistral-7B-Instruct-v0.2',]\n",
    "\n",
    "topic = 'gsm8k'\n",
    "\n",
    "evaluator = 'gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = load_batches(topic, models)\n",
    "# all_cards = load_cards(topic, 'dict', models, evaluator)\n",
    "all_completions = {k: get_all_completions(v, paraphrase) for k,v in all_batches.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}\n",
    "for model in models:\n",
    "    all_embeddings[model] = get_all_embeddings(all_completions[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "# make director if not exist\n",
    "\n",
    "for model in models:\n",
    "    if not os.path.exists(f'../code/embeddings/mmlu/{topic}'):\n",
    "        os.makedirs(f'../code/embeddings/mmlu/{topic}')\n",
    "\n",
    "    np.save(f'../code/embeddings/mmlu/{topic}/{model}{\"-paraphrase\" if paraphrase else \"\"}-test.npy', all_embeddings[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = load_batches(topic, models, 'train')\n",
    "# all_cards = load_cards(topic, 'dict', models, evaluator)\n",
    "all_completions = {k: get_all_completions(v) for k,v in all_batches.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}\n",
    "for model in models:\n",
    "    all_embeddings[model] = get_all_embeddings(all_completions[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "# make director if not exist\n",
    "\n",
    "for model in models:\n",
    "    if not os.path.exists(f'../code/embeddings/mmlu/{topic}'):\n",
    "        os.makedirs(f'../code/embeddings/mmlu/{topic}')\n",
    "\n",
    "    np.save(f'../code/embeddings/mmlu/{topic}/{model}{\"-paraphrase\" if paraphrase else \"\"}-train.npy', all_embeddings[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_completions[models[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
