{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "from concurrent.futures import wait\n",
    "\n",
    "from eval_fully_contrastive import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from few_shot import *\n",
    "from generative_method import *\n",
    "import matplotlib.pyplot as plt\n",
    "from card import GenerativeCard"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "card_format = 'dict'\n",
    "\n",
    "initial_criteria = []\n",
    "baseline = 'card'\n",
    "\n",
    "model = 'Llama-2-13b-chat-hf'\n",
    "# model = 'Mixtral-8x7B-Instruct-v0.1'\n",
    "topic = 'high_school_chemistry'\n",
    "folder_path = '03-14_21-59-48_Llama-2-13b-chat-hf_main'\n",
    "if card_format == 'dict':\n",
    "    initial_criteria = get_initial_criteria(topic)\n",
    "\n",
    "assert model in folder_path"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "cm = CostManager()\n",
    "exp = 'web_eval' + topic\n",
    "\n",
    "hp = {\n",
    "            # general\n",
    "            'experiment'      : exp,\n",
    "            'name'            : f'{model}_main',\n",
    "            'method'          : 'generative',\n",
    "            'load_from'       : None,\n",
    "            # dataset\n",
    "            'dataset_folder'  : 'datasets/mmlu',\n",
    "            'topic'           : topic,\n",
    "            'shuffle'         : False,\n",
    "            'seed'            : 311,\n",
    "            'batch_nums'      : [8] * 5 + [60],\n",
    "            'model'           : model,\n",
    "            # training\n",
    "            'epoch'           : None,  # None: use the number of training batches as epoch\n",
    "            'use_refine'      : False,\n",
    "            'initial_criteria': initial_criteria,\n",
    "            'CoT'             : False,\n",
    "            'card_format'     : card_format,\n",
    "            # eval\n",
    "            'async_eval'      : True,\n",
    "        }\n",
    "m =  GenerativeMethod(hp, cm)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "print(m.model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "no_to_letter = {0: 'A', 1: 'B', 2: 'C', 3: 'D'} #  0-3: A-D, ALL OTHERS: N\n",
    "\n",
    "def format_single_entry(entry):\n",
    "    q, choice, gt, pred, completion = entry\n",
    "\n",
    "    choice_str = '\\n'.join([f'{no_to_letter[i]}: {c}' for i, c in enumerate(choice)])\n",
    "    gt_str = no_to_letter[gt] if gt in no_to_letter else 'N'\n",
    "\n",
    "    return f'{q}\\nChoices:\\n{choice_str}\\nCorrect answer: {gt_str}'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def format_few_shot_entry(entry):\n",
    "    q, choice, gt, pred, completion = entry\n",
    "\n",
    "    choice_str = '\\n'.join([f'{no_to_letter[i]}: {c}' for i, c in enumerate(choice)])\n",
    "    gt_str = no_to_letter[gt] if gt in no_to_letter else 'N'\n",
    "    student_correctness = gt == pred\n",
    "\n",
    "    return f'{q}\\nChoices:\\n{choice_str}\\nStudent Completion: {completion}\\nGround Truth Answer: {gt_str}\\nStudents correctness: {student_correctness}'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "def format_full_eval_str(method):\n",
    "    str_all = ''\n",
    "    cnt = 1\n",
    "    for entry in method.testing_batch:\n",
    "        entry_formatted = (format_single_entry(entry))\n",
    "        str_all += f'Question {cnt}. {entry_formatted}\\n\\n'\n",
    "        cnt += 1\n",
    "    \n",
    "    return str_all\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def format_few_shot_string(method):\n",
    "    str_all = ''\n",
    "    cnt = 1\n",
    "    for i in range(len(method.training_batches)):\n",
    "        for entry in method.training_batches[i]:\n",
    "            entry_formatted = (format_few_shot_entry(entry))\n",
    "            str_all += f'Few shot sample question {cnt}. {entry_formatted}\\n\\n'\n",
    "            cnt += 1\n",
    "    \n",
    "    return str_all\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def get_training_accuracy(method):\n",
    "    correct_cnt = 0\n",
    "    tot_cnt = 0\n",
    "    for i in range(len(method.training_batches)):\n",
    "        for entry in method.training_batches[i]:\n",
    "            q, choices, gt, pred, completion = entry\n",
    "            if gt == pred:\n",
    "                correct_cnt += 1\n",
    "            tot_cnt += 1\n",
    "    \n",
    "    # 2 Decimal \n",
    "    return round(correct_cnt / tot_cnt, 2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "if card_format != 'str':\n",
    "    card_path = f'./outputs/generative_{topic}/{folder_path}/cards/epoch_4_card.json'\n",
    "else:\n",
    "    card_path = f'./outputs/generative_{card_format}_{topic}/{folder_path}/cards/epoch_4_card.json'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "card_path = f'./outputs/generative_{card_format}_{topic}/{folder_path}/cards/epoch_4_card.json'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "with open(card_path) as f:\n",
    "    card_dict = json.load(f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "if card_format == 'dict':\n",
    "    card = GenerativeCard(d=card_dict)\n",
    "elif card_format == 'bullet_point':\n",
    "    card = '\\n -'.join([card_dict['card'][str(i)] for i in range(len(card_dict['card']))])\n",
    "else:\n",
    "    card = card_dict['card']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "if baseline == 'few_shot':\n",
    "    few_shot_str = format_few_shot_string(m)\n",
    "    card = few_shot_str"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "str(card)[:50]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "card = str(card)\n",
    "# card += f\"\\n# LLM Student's Past Performance:\\nThe LLM student's accuracy on previous questions under this topic: {get_training_accuracy(m)}.\\n\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "get_training_accuracy(m)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "str_all = format_full_eval_str(m)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "with open('prompts/eval/predictive/system_mmlu.txt', 'r') as f:\n",
    "   sys_prompt = f.read() \n",
    "\n",
    "with open('prompts/eval/predictive/user.txt', 'r') as f:\n",
    "   user_prompt = f.read() "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "sys_prompt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "user_prompt = \"\"\"\n",
    "Predict if the Large Language Model (LLM) student will answer The Question correctly based on the Student Evaluation. You will do this by reasoning step-by-step:\n",
    "\n",
    "# Instruction:\n",
    "\n",
    "1. Analyze how the student may answer the questions based on the criteria.\n",
    "2. Based on your analysis, predict if the student will answer The Question correctly (true) or not (false).\n",
    "3. Note that the student is an LLM, so their performance would not change over time.\n",
    "\n",
    "Requirement: Don't make any assumptions about the student. Your prediction should be solely grounded on the Student Evaluation below.\n",
    "\n",
    "## The Questions\n",
    "\n",
    "{qa}\n",
    "\n",
    "## Student Evaluation\n",
    "\n",
    "{card}\n",
    "# Formatting:\n",
    "You should do one single aggregated analysis, and then output verdict one by one. \n",
    "Your task is not to establish expectations for the LLM student but to predict the student's performance based on the evaluation. So you should be objective and unbiased.\n",
    "\n",
    "Provide an analysis using the format:\n",
    "# Aggregated analysis: <student's capability>\n",
    "...\n",
    "\n",
    "Make predictions one by one, using the format:\n",
    "Question index: Verdict\n",
    "\n",
    "<1-60>: <T/F>\n",
    "\"\"\"\n",
    "\n",
    "if baseline == 'few_shot':\n",
    "    user_prompt += \"Note that questions under student evaluation are examples for you to check, the first 60 questions are what you are going to predict!\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "formatted_str = f\"\"\"{sys_prompt.format(topic=topic)}\\n\\n{user_prompt.format(qa=str_all, card=str(card))}\"\"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "if baseline != 'few_shot':\n",
    "    with open('Web/eval_input.txt', 'w') as f:\n",
    "        f.write(formatted_str)  \n",
    "else:\n",
    "    with open('Web/eval_few_shot.txt', 'w') as f:\n",
    "        f.write(formatted_str)  "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "ground_truths = [entry[2] == entry[3] for entry in m.testing_batch]\n",
    "\n",
    "gt_str = ''\n",
    "cnt = 1\n",
    "for g in ground_truths:\n",
    "    gt_str += f'{cnt}. {g}\\n'\n",
    "    cnt += 1\n",
    "\n",
    "with open('Web/eval_gt.txt', 'w') as f:\n",
    "    f.write(gt_str)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "print(f'Accuracy is {sum(ground_truths) / len(ground_truths)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "def count_words(txt):\n",
    "    return len(re.findall(r'\\w+', txt))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "with open('Web/eval_output.txt', 'r') as f:\n",
    "    lines = f.readlines()[-60:]\n",
    "\n",
    "outputs = []\n",
    "for l in lines:\n",
    "    match = re.search(r': (.*)', l)\n",
    "    # match = re.search(r'Verdict: (.*)', l)\n",
    "    if match:\n",
    "        outputs.append(match.group(1))\n",
    "\n",
    "def tf_extractor(outputs):\n",
    "    return [o[0] for o in outputs]\n",
    "\n",
    "def true_false_extractor(outputs):\n",
    "    return [bool(o) for o in outputs]\n",
    "\n",
    "outputs = tf_extractor(outputs)\n",
    "\n",
    "correct_cnt = 0\n",
    "for i in range(len(outputs)):\n",
    "   correct = ground_truths[i] == (outputs[i] == 'T')\n",
    "   correct_cnt += correct\n",
    "\n",
    "assert len(outputs) == len(ground_truths)\n",
    "\n",
    "print(f'Claude 3 accuracy: {correct_cnt / len(outputs)}')\n",
    "oracle = sum(ground_truths) / len(ground_truths)\n",
    "print(f'Oracle accuracy is {max(oracle, 1-oracle)}')\n",
    "if card_format != 'str':\n",
    "    mixtral_path = f'./outputs/generative_{topic}/{folder_path}/eval_test_epoch_4_predictive.json'\n",
    "else:\n",
    "    mixtral_path = f'./outputs/generative_{card_format}_{topic}/{folder_path}/eval_test_epoch_4_predictive.json'\n",
    "\n",
    "mixtral_path = f'./outputs/generative_{card_format}_{topic}/{folder_path}/eval_test_epoch_4_predictive.json'\n",
    "with open(mixtral_path) as f:\n",
    "    mixtral_outputs = json.load(f)\n",
    "mixtral_accuracy = mixtral_outputs['metrics']['accuracies']\n",
    "print(f'Mixtral accuracy: {mixtral_accuracy[0]}')\n",
    "\n",
    "print(f'Word count: {count_words(str(card))}')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "# Do a confusion matrix\n",
    "\n",
    "\n",
    "outputs = [outputs[i] == 'T' for i in range(len(outputs))]\n",
    "\n",
    "cm = confusion_matrix(ground_truths, true_false_extractor(outputs))\n",
    "# plot with legend, label, for each block, label with the number\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(cm, cmap='Blues')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + ['F', 'T'])\n",
    "ax.set_yticklabels([''] + ['F', 'T'])\n",
    "for (i, j), val in np.ndenumerate(cm):\n",
    "    ax.text(j, i, f'{val}', ha='center', va='center')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix on model {model}\\n for topic {topic}')\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
