{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:58:35.475738Z",
     "start_time": "2024-05-05T19:58:34.840274Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "initial data reading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e58cb0227d9a986e"
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(\"test.parquet\")\n",
    "shuffled = df.sample(frac=1).reset_index(drop=True)[:200]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:23:39.334377Z",
     "start_time": "2024-05-05T19:23:38.899128Z"
    }
   },
   "id": "6a9f26dca9824673",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_qa = []\n",
    "for _, row in shuffled.iterrows():\n",
    "    all_qa.append(\n",
    "        {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "        }\n",
    "    )\n",
    "pprint(all_qa[:1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:23:39.342922Z",
     "start_time": "2024-05-05T19:23:39.335464Z"
    }
   },
   "id": "1063ea903ab71ca8",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "model specification and helper functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "739c227ef844ebc"
  },
  {
   "cell_type": "code",
   "source": [
    "from models import select_model\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You always give perfect reasoning and correct answers.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Please solve the following problem step by step and provide a detailed reasoning for your answer.\n",
    "{q}\n",
    "\"\"\"\n",
    "\n",
    "judger_system = \"\"\"\n",
    "You can perfectly determine if a student answer a question correctly or not.\n",
    "\"\"\"\n",
    "\n",
    "judger_model = select_model(\"meta-llama/Meta-Llama-3-70B-Instruct\", judger_system)\n",
    "\n",
    "judger_user = \"\"\"\n",
    "Determine if the student correctly answer the given Question given the ground truth answer.\n",
    "IMPORTANT: Only if the student's final answer is consistent with the ground truth answer, you should mark it as correct.\n",
    "\n",
    "Submit your judgement using the following JSON format:\n",
    "{{\n",
    "    \"Is the student's answer correct?\": true or false\n",
    "}}\n",
    "\n",
    "# Question\n",
    "\n",
    "{q}\n",
    "\n",
    "# Ground Truth Answer\n",
    "\n",
    "{ga}\n",
    "\n",
    "# Student's Answer\n",
    "\n",
    "{sa}\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(question: str, answer: str, model_name: str):\n",
    "    try:\n",
    "        model = select_model(model_name, system_prompt)\n",
    "        sa = model(user_prompt.format(q=question), temperature=0.0)\n",
    "        judger_model.clear_conversations()\n",
    "        correct = judger_model(judger_user.format(q=question, ga=answer, sa=sa), use_json=True)[\"Is the student's answer correct?\"]\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": answer,\n",
    "            \"model_answer\": sa,\n",
    "            \"correct\": correct\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def judge(question: str, true_answer: str, student_answer: str):\n",
    "    judger_model.clear_conversations()\n",
    "    correct = judger_model(judger_user.format(q=question, ga=true_answer, sa=student_answer), use_json=True)[\"Is the student's answer correct?\"]\n",
    "    return correct"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T20:05:24.292578Z",
     "start_time": "2024-05-05T20:05:21.675665Z"
    }
   },
   "id": "290d3b81527354d0",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "peak one sample and test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9531c79d487e5a48"
  },
  {
   "cell_type": "code",
   "source": [
    "qa = all_qa[0]\n",
    "generate_answer(qa[\"question\"], qa[\"answer\"], \"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:24:48.918488Z",
     "start_time": "2024-05-05T19:24:35.168184Z"
    }
   },
   "id": "88a23f82cb9bfe38",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "load existing questions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec1580939319c611"
  },
  {
   "cell_type": "code",
   "source": [
    "# read one file for all qa\n",
    "all_qa = []\n",
    "file = \"Meta-Llama-3-8B-Instruct-all.jsonl\"\n",
    "with jsonlines.open(file, \"r\") as reader:\n",
    "    for json in reader:\n",
    "        all_qa.append({\n",
    "            \"question\": json[\"question\"],\n",
    "            \"answer\": json[\"true_answer\"]\n",
    "        })\n",
    "print(len(all_qa))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T20:05:12.392917Z",
     "start_time": "2024-05-05T20:05:12.383633Z"
    }
   },
   "id": "edf8dd7da8f18e0f",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate data for all questions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a880d2cadfb4fc7"
  },
  {
   "cell_type": "code",
   "source": [
    "student_models = [\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    # \"gpt-3.5-turbo\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "]\n",
    "\n",
    "for student_model in tqdm(student_models):\n",
    "    results = []\n",
    "    existing_qs = set()\n",
    "    splits = student_model.split(\"/\")\n",
    "    if len(splits) > 1:\n",
    "        model_name = splits[1]\n",
    "    else:\n",
    "        model_name = splits[0]\n",
    "    if os.path.exists(f\"{model_name}-all.jsonl\"):\n",
    "        # read existing questions\n",
    "        with jsonlines.open(f\"{model_name}-all.jsonl\", \"r\") as reader:\n",
    "            for json in reader:\n",
    "                existing_qs.add(json[\"question\"])\n",
    "                results.append(json)\n",
    "        print(f\"Existing questions: {len(existing_qs)}\")\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = [executor.submit(generate_answer, qa[\"question\"], qa[\"answer\"], student_model) for qa in all_qa if qa[\"question\"] not in existing_qs]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            r = future.result()\n",
    "            if r is not None:\n",
    "                results.append(r)\n",
    "\n",
    "    with jsonlines.open(f\"{model_name}-all.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T20:16:16.797385Z",
     "start_time": "2024-05-05T20:05:24.293567Z"
    }
   },
   "id": "58d1a7824e9dd27b",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "reorder and sanity checks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3965bcf6af9ccae8"
  },
  {
   "cell_type": "code",
   "source": [
    "files = [\n",
    "    \"Meta-Llama-3-8B-Instruct-all.jsonl\",\n",
    "    \"Meta-Llama-3-70B-Instruct-all.jsonl\",\n",
    "    \"Llama-2-13b-chat-hf-all.jsonl\",\n",
    "    \"Llama-2-70b-chat-hf-all.jsonl\",\n",
    "    \"gpt-3.5-turbo-all.jsonl\",\n",
    "    \"Mistral-7B-Instruct-v0.2-all.jsonl\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1-all.jsonl\",\n",
    "]\n",
    "\n",
    "jsons = []\n",
    "for f in files:\n",
    "    j = []\n",
    "    with jsonlines.open(f, \"r\") as reader:\n",
    "        for line in reader:\n",
    "            j.append(line)\n",
    "    jsons.append(j)\n",
    "\n",
    "# sort by question\n",
    "for j in jsons:\n",
    "    j.sort(key=lambda x: x[\"question\"])\n",
    "\n",
    "# shuffle each json with the same seed to ensure the same order\n",
    "seed = 42\n",
    "for i, j in enumerate(jsons):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(j)\n",
    "\n",
    "# check that all jsons are in the same order\n",
    "for i in range(1, len(jsons)):\n",
    "    assert [qa[\"question\"] for qa in jsons[0]] == [qa[\"question\"] for qa in jsons[i]]\n",
    "\n",
    "# write to new files\n",
    "for i, j in enumerate(jsons):\n",
    "    with jsonlines.open(files[i], \"w\") as writer:\n",
    "        writer.write_all(j)\n",
    "\n",
    "# split into train and test\n",
    "for i, j in enumerate(jsons):\n",
    "    with jsonlines.open(files[i].replace(\"all\", \"train\"), \"w\") as writer:\n",
    "        writer.write_all(j[:40])\n",
    "    with jsonlines.open(files[i].replace(\"all\", \"test\"), \"w\") as writer:\n",
    "        writer.write_all(j[40:100])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T20:20:35.881100Z",
     "start_time": "2024-05-05T20:20:35.737684Z"
    }
   },
   "id": "ed1dadb0d7a3e4ff",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# redo judge\n",
    "files = [\n",
    "    # \"Meta-Llama-3-8B-Instruct-all.jsonl\",\n",
    "    # \"Meta-Llama-3-70B-Instruct-all.jsonl\",\n",
    "    \"Llama-2-13b-chat-hf-all.jsonl\",\n",
    "    \"Llama-2-70b-chat-hf-all.jsonl\",\n",
    "]\n",
    "\n",
    "for f in tqdm(files):\n",
    "    with jsonlines.open(f, \"r\") as reader:\n",
    "        results = []\n",
    "        for qa in tqdm(reader):\n",
    "            correct = judge(qa[\"question\"], qa[\"true_answer\"], qa[\"model_answer\"])\n",
    "            qa[\"correct\"] = correct\n",
    "            results.append(qa)\n",
    "    with jsonlines.open(f, \"w\") as writer:\n",
    "        writer.write_all(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T03:12:57.278825Z",
     "start_time": "2024-05-05T03:01:46.053424Z"
    }
   },
   "id": "e6e5bad0361dcde0",
   "execution_count": 5,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
